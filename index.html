<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Dolphins"/>
  <meta property="og:description" content="Dolphins: A Multi-Modal Driving Model for Zero and Few-shot (In-context) Learning"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  



  <meta name="twitter:title" content="Dolphins">
  <meta name="twitter:description" content="Dolphins: A Multi-Modal Driving Model for Zero and Few-shot (In-context) Learning">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VLM-Driver</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">-->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Dolphins: A Multi-Modal Driving Model for Zero and Few-shot (In-context) Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
            <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://gray311.github.io/" target="_blank">Yingzi Ma<sup>*</sup></a><sup>1,2</sup>,&nbsp;</span>
                <!--<span class="author-block">
                  <a href="https://sites.google.com/view/hbansal" target="_blank">Hritik Bansal<sup>*</sup></a><sup>3</sup>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://jmhessel.com/" target="_blank">Jack Hessel<sup>*</sup></a><sup>4</sup>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://rulinshao.github.io/" target="_blank">Rulin Shao</a><sup>5</sup>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://wanrong-zhu.com/" target="_blank">Wanrong Zhu</a><sup>6</sup>,&nbsp;</span><br>
                <span class="author-block">
                  <a href="https://anas-awadalla.streamlit.app/" target="_blank">Anas Awadalla</a><sup>5</sup>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://homes.cs.washington.edu/~jpgard/" target="_blank">Josh Gardner</a><sup>5</sup>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://www.rohantaori.com/" target="_blank">Rohan Taori</a><sup>7</sup>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://people.csail.mit.edu/ludwigs/" target="_blank">Ludwig Schmidt</a><sup>4,5,8</sup>&nbsp;</span>-->
                </div>

                <div class="is-size-5 publication-authors">
                  <span class="author-block"><sup>1</sup> Sichuan University,&nbsp;&nbsp;</span>
                  <!--<span class="author-block"><sup>2</sup> Google Research,&nbsp;&nbsp;</span>    
                  <span class="author-block"><sup>3</sup> University of California Los Angeles,&nbsp;&nbsp;</span>
                  <span class="author-block"><sup>4</sup> Allen Institute for AI,&nbsp;&nbsp;</span>
                  <span class="author-block"><sup>5</sup> University of Washington,&nbsp;&nbsp;</span>
                  <span class="author-block"><sup>6</sup> University of California, Santa Barbara,&nbsp;&nbsp;</span>
                  <span class="author-block"><sup>7</sup> Stanford University&nbsp;&nbsp;</span>
                  <span class="author-block"><sup>8</sup> LAION</span>
                  <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>-->
                </div>


                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://github.com/vlm-driver/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/vlm-driver/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>


                <!-- HuggingFace Link -->
                <!--<span class="link-block">
                  <a href=" https://huggingface.co/datasets/mlfoundations/VisIT-Bench" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:20px">&#x1F917;</p>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>-->

              <!-- HuggingFace Link -->
                <!-- <span class="link-block">
                  <a href=" https://huggingface.co/spaces/mlfoundations/VisIT-Bench-Leaderboard" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:20px">&#x1F917;</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>-->

              <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://github.com/vlm-driver/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Video Demo</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="publication-flipped">
        <div class="content has-text-justified">
            <p style="text-align:center;">
                <iframe align="middle" width="700" height="400" src="https://www.youtube.com/embed/Pbzn79TSRO0?si=1tCNSZBb-jEx3wYl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
               <br>
            </p>

        </div>
      </div>
    </div>
     
    <!-- <h3 class="subtitle is-size-4-tablet has-text-left pb-">
    <p style="text-align:justify">
      Extended from OpenFlamingo, <b><em>Dolphins</em></b> is a state-of-the-art Large Vision-Language Model (LVLM) tailored for Autonomous Vehicles (AVs) with in-context learning ability. The salient features of <b><em>Dolphins</em></b> encapsulate diverse and comprehensive tasks including:<b>A Comprehensive Instruction Model</b>, <b>Multi-Round Reflection Reasoning</b>, and <b>Ego-Centric Contingency Planning</b></p>
  </div><h3>-->
</section>



<!-- Paper abstract -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="publication-flipped">
        <div class="content has-text-justified">
            <h3 class="subtitle is-size-3-tablet has-text-left pb-3">
               <p align="center">
                   <br>
                    Abstract<br>
               </p>
            </h3>
            <h3 class="subtitle is-size-5-tablet has-text-left pb-6">
               <p style="text-align:justify; line-height:150%">
                   The quest for fully autonomous vehicles (AVs) capable of navigating complex real-world scenarios with human-like understanding and responsiveness necessitates the convergence of advancements in artificial intelligence, robotics, and automotive engineering. In this paper, we introduce Dolphins, a novel vision language model architected to imbibe human-like driving abilities. Dolphins is adept at processing multimodal inputs comprising video (or image) data, text instructions, and historical control signals to generate informed outputs corresponding to the provided instructions. Building upon the open-sourced pretrained Vision Language Model, OpenFlamingo, we tailored Dolphins to the driving domain by constructing driving-specific instruction data and conducting instruction tuning. Through the utilization of the BDD-X dataset, we designed and consolidated four distinct AV tasks into Dolphins to foster a holistic understanding of intricate driving scenarios. The distinctive features of Dolphins are delineated into two primary dimensions: (1) the ability to provide a comprehensive understanding of complex and long-tailed open-world driving scenarios and solve a spectrum of AV tasks, and (2) the emergence of human-like capabilities including gradient-free rapid learning and adaptation (in-context learning), reflection and error recovery, and interpretability. 
Through an extensive evaluation on the BDD-X dataset, Dolphins demonstrates an emergent ability to generalize across a variety of AV tasks, enabling it to proffer fine-grained scene understanding including but not limited to various road agent attributes (e.g., a silver car with right turn light on), behaviors (e.g., yielding to pedestrians crossing the street), environmental conditions (e.g., snowy day on a busy road in a city), and to forge robust planning strategies.
Moreover, Dolphins's in-context learning ability, reflection and error recovery mechanisms, and interactive conversational interface foster a robust, reliable, and user-trustworthy autonomous driving system, shedding lights on what future AVs can achieve. Through Dolphins, we envisage a substantial stride towards bridging the understanding gap between humans and autonomous driving systems, propelling the AV domain closer to realizing fully autonomous vehicles.
               </p>
            </h3>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Perception (Scenario Understanding)
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
             An example showcasing <em><b>Dolphins</b></em>’s capability in scenario understanding. The video features an ego car driving in a tunnel. <em><b>Dolphins</b></em> can identify the environment in which the ego car is situated and accurately determine the color of the front vehicle as well as infer the current time.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/perception/1.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Perception (Scenario Understanding)
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
             An example showcasing <em><b>Dolphins</b></em>’s capability in scenario understanding. The video features an ego car driving on a snowy street. <em><b>Dolphins</b></em> can identify the environment in which the ego car is situated, the presence of the traffic light, and accurately determine the color of the passing vehicle.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/perception/2.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Perception (Behavior Understanding)
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
            An example showcasing <em><b>Dolphins</b></em>’s capability in scenario understanding and behavior understanding. The video features an ego car stopping at an intersection on a rainy day. <em><b>Dolphins</b></em> comprehensively describes the environment in which the ego car is situated, the behavior of the ego car, and can infer the reasons for its behavior.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/perception/3.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Perception (Behavior Understanding)
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
            An example showcasing <em><b>Dolphins</b></em>’s capability in scenario understanding and behavior understanding. The video features an ego car making a right turn. <em><b>Dolphins</b></em> can identify these contents. <s>Words</s> means hallucination.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/perception/4.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Perception (Behavior Understanding)
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
            An example showcasing <em><b>Dolphins</b></em>’s capability in scenario understanding and behavior understanding. The video shows an ego car following a taxi and going through an intersection. <s>Words</s> means hallucination.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/perception/5.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Perception (Behavior Understanding)
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
            An example showcasing <em><b>Dolphins</b></em>’s capability in scenario understanding and behavior understanding. The video shows an ego car driving slowly on a busy road at night. <em><b>Dolphins</b></em> can identify the ego car traveling at a slow speed and infer that the reason is that the speed of the vehicle ahead is restricting the ego car’s speed.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/perception/6.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- Image carouse2 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Prediction
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
             An example showcasing <em><b>Dolphins</b></em>’s capability in scenario understanding. The video features an ego car driving in a tunnel. <em><b>Dolphins</b></em> can identify the environment in which the ego car is situated and accurately determine the color of the front vehicle as well as infer the current time.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/prediction/8.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Prediction
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
             An example showcasing <em><b>Dolphins</b></em>’s capability in scenario understanding. The video features an ego car driving on a snowy street. <em><b>Dolphins</b></em> can identify the environment in which the ego car is situated, the presence of the traffic light, and accurately determine the color of the passing vehicle.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/prediction/9.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

  </div>
</div>
</div>
</section>
<!-- End image carouse2 -->


<!-- Image carouse3 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Planning
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
             An example showcasing <em><b>Dolphins</b></em>’s capability in scenario understanding. The video features an ego car driving in a tunnel. <em><b>Dolphins</b></em> can identify the environment in which the ego car is situated and accurately determine the color of the front vehicle as well as infer the current time.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/planning/16.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Planning
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
             An example showcasing <em><b>Dolphins</b></em>’s capability in scenario understanding. The video features an ego car driving on a snowy street. <em><b>Dolphins</b></em> can identify the environment in which the ego car is situated, the presence of the traffic light, and accurately determine the color of the passing vehicle.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/planning/17.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

  </div>
</div>
</div>
</section>
<!-- End image carouse3 -->


<!-- Image carouse4 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

         <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Rapid Learning and Adaptation (In-context Learning)
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
            three examples show our model enables rapid adaptation to unseen instructions through in-context learning. In the first two examples, <em><b>Dolphins</b></em> learns to play the role of the driver through in-context examples and can accurately describe its behavior, despite not having been trained on such instructions. The third example shows that <em><b>Dolphins</b></em> can learn common sense from in-context examples, such as not being able to judge the current time based on the light when inside a tunnel.
         </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/perception/7.png"  style="width: 100%; height: 100%"/>
         </p>
         </h3>
      </div>

  </div>
</div>
</div>
</section>
<!-- End image carouse4 -->



<!--<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.16.2/gradio.js"></script>-->
<!-- Youtube video -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--          <div class="publication-video">-->
<!--          <h3 class="subtitle is-size-4-tablet has-text-left">-->
<!--               <p>-->
<!--                   q2d's auto-generated dialogs enable query generation models to adapt and improve for specific dialog styles, creating labeled datasets for training and evaluation.-->
<!--                   <br>T5 model predictions above/below the line show the impact of fine-tuning on MuSiQue dialogs.-->
<!--               </p>-->
<!--               <p style="text-align:center;">-->
<!--                   <br><br>-->
<!--                   <img src="static/images/q2d_5.png"  style="width: 60%; height: 60%"/>-->
<!--               </p>-->
<!--         </h3>-->
<!--          </div>-->
<!--        </div>-->

<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!-- End youtube video-->

<!-- Youtube video -->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h3 class="subtitle is-size-4-tablet has-background-info-light has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--      We collect <i>normal</i> (synthetic, not weird) and <i>natural</i> (non-synthetic, not weird) images to investigate the main challenge in WHOOPS!. BLIP2 model performs well on <i>non-weird</i> cases but struggles on weird ones, indicating that weirdness is the primary challenge, not synthesis.-->
<!--      </h3>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--          <div class="publication-video">-->
<!--            <iframe src="https://nlphuji-wmtis-explorer-identify.hf.space" frameborder="0" width="850" height="450"></iframe>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->


<!-- Paper poster -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Paper</h2>-->
<!--      <iframe  src="static/pdfs/WHOOPS_paper.pdf" width="100%" height="550">-->
<!--          </iframe>-->
<!--        -->
<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--End paper poster -->

<!--BibTex citation -->
<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
<h2 class="title">BibTeX</h2>
<pre><code>@misc{bitton2023visitbench,
      title={VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use}, 
      author={Yonatan Bitton and Hritik Bansal and Jack Hessel and Rulin Shao and Wanrong Zhu and Anas Awadalla and Josh Gardner and Rohan Taori and Ludwig Schimdt},
      year={2023},
      eprint={2308.06595},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}</code>
</div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p style="color:gray;font-size:9.9px;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
  </html>
